{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sp\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = pd.read_csv('../DL_and_NN_in_Python/fer2013.csv')\n",
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def show_sample_image(emotion: int):\n",
    "    emotion_no = emotions.index(emotion)\n",
    "    df_emotion = df[df.emotion == emotion_no]\n",
    "\n",
    "    random_no = np.random.random_integers(0, len(df_emotion))\n",
    "    print(random_no)\n",
    "    print(df_emotion.iloc[random_no])\n",
    "\n",
    "    img = np.array(list(map(int, df_emotion.iloc[random_no].pixels.split(' '))), dtype=np.uint8).reshape((48,48))\n",
    "    img = PIL.Image.fromarray(img).resize((1000, 1000))\n",
    "    img.show()\n",
    "\n",
    "show_sample_image('Surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.8\n",
    "train_index = int(train_proportion*len(df))\n",
    "\n",
    "train_df = df.iloc[:train_index]\n",
    "test_df = df.iloc[train_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_df)\n",
    "D = len(train_df.iloc[0].pixels.split(' '))\n",
    "D1 = int(np.sqrt(D))\n",
    "\n",
    "# or just use train_test_split from sklearn.model_selection for the same effect\n",
    "n_classes = len(set(train_df.emotion))\n",
    "\n",
    "print(f'N = {N}, D = {D}, n_classes: {n_classes}')\n",
    "\n",
    "print(f'Number of samples in training set: {len(train_df)}')\n",
    "print(f'Number of samples in test set: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        image = self.data.iloc[idx, 1]\n",
    "        image = list((map(int, image.split(' '))))\n",
    "        image = np.array(image, dtype=np.uint8) # dtype needs to be np.uint8, otherwise ToTensor() won't scale pixel values to [0.0, 1.0]\n",
    "        image = image.reshape((D1, D1)) # reshape because ToTensor() expects a 2D image\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = CustomImageDataset(train_df, transform=ToTensor())\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = CustomImageDataset(test_df, transform=ToTensor())\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind creating a model in PyTorch is pretty much the same as in Tensorflow. There are some syntactical differences, though.\n",
    "\n",
    "1. The activation function is not specified as part of the layer, but separately.\n",
    "2. The input and output dimensions need to be supplied, unlike in Keras, where only output is needed. Note that in pure TF, without Keras, we also need to specify the intput dims unless we postpone the build of the model (see https://www.tensorflow.org/guide/intro_to_modules#waiting_to_create_variables)\n",
    "\n",
    "    See also this package which can directly infer layer sizes upon passing sample input: https://github.com/szymonmaszke/torchlayers\n",
    "3. The input has to be explicitly turned into a torch.Tensor (TF can handle that automatically)\n",
    "4. No explicit ```.fit``` function, need to write the training loop ourselves\n",
    "5. Custom weight initalisation is not so straightforward\n",
    "6. L2 regularisation is added to the optimiser, not to the layer. It is usally referred to as ```weight_decay```:\n",
    "\n",
    "    https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch\n",
    "    https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "\n",
    "    L1 reuglarisation is harder but can be implemented by creating a custom layer:\n",
    "    https://stackoverflow.com/a/66630301\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPyTorch(torch.nn.Module):\n",
    "    def __init__(self, M: int = 10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(D1*D1, M),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M, n_classes)\n",
    "        )\n",
    "        self.float() # ensures input has the same dtype as the parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPyTorchV2(nn.Module):\n",
    "    def __init__(self, M: int = 10, activation_function = nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(D1*D1, M)\n",
    "        self.layer2 = nn.Linear(M, n_classes)\n",
    "        self.activation_function = activation_function\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation_function(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructNNPyTorch(M: int = 10, activation_function = nn.ReLU()):\n",
    "    model = nn.Sequential()\n",
    "    model.add_module('flatten', nn.Flatten())\n",
    "    model.add_module('name1', nn.Linear(D, M))\n",
    "    model.add_module('name2', activation_function)\n",
    "    model.add_module('name3', nn.Linear(M, n_classes))\n",
    "    model.float()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalise the model an print some information about it. Note that no ```model.compile``` is needed in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = ModelPyTorch().to(device)\n",
    "print(model_v1)\n",
    "model_v2 = ModelPyTorchV2().to(device)\n",
    "print(model_v2)\n",
    "model_v3 = constructNNPyTorch(M=10).to(device)\n",
    "print(model_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom weight initialisation is not as straightforward as in Keras.\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.init.html\n",
    "- https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes in a module and applies the specified weight initialization\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "        values taken from a normal distribution.'''\n",
    "\n",
    "    # for every Linear layer in a model\n",
    "    if isinstance(m, nn.Linear):\n",
    "        fan_in = m.in_features\n",
    "\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 1/np.sqrt(fan_in))\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        # either of these works\n",
    "        # m.bias.data.fill_(0)\n",
    "        # m.weight.data.normal_(0.0,1/np.sqrt(fan_in)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('biases before custom initalisation:\\n', [module.bias.data for module in model_v1.modules() if isinstance(module, nn.Linear)])\n",
    "model_v1.apply(weights_init_normal)\n",
    "print('and after:\\n', [module.bias.data for module in model_v1.modules() if isinstance(module, nn.Linear)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, D1, D1, device=device, dtype=torch.float)\n",
    "logits = model_v1(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print model's parameters. Note that the weights and biases at each layer are automatically given ```requires_grad=True```. If we constructed a computational graph by hand, i.e. without using Sequential, we would have to pass manually specify which parameters 'require grad', i.e. should be included in backpropagation. Switching off gradient tracking can also be done with ```torch.no_grad()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_v1.named_parameters():\n",
    "    print(name)\n",
    "    print(param.size())\n",
    "    print(param)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to TF, define a loss function and an optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model_v1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step and the loop have to be defined manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, loss_function, optimiser):\n",
    "    # sets the model to the triaing mode\n",
    "    # this is only important if we have batch normalisation or dropout\n",
    "    # equivalent to TF's training=True option\n",
    "    model.train() \n",
    "\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        loss = loss_function(model(X), Y)\n",
    "\n",
    "        # calculate the gradient and apply it to the parameters\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad() # need to reset the gradient\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss_val = loss.item()\n",
    "            current = batch*batch_size + len(X)\n",
    "            print(f'Current batch loss: {loss_val}')\n",
    "            print(f'Current size: {current}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(dataloader, model, loss_function):\n",
    "    # now set the model to evaluation mode to avoid dropout etc.\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # do not compute gradients, because they are not needed at the inference stage\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, Y).item()\n",
    "            correct += (pred.argmax(axis=1) == Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    training_step(train_dataloader, model_v1, loss_function, optimiser)\n",
    "    test_step(test_dataloader, model_v1, loss_function)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
