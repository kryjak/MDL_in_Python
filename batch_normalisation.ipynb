{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sp\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from create_spiral import create_spiral\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of GPUs detected: {len(tf.config.list_logical_devices('GPU'))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../DL_and_NN_in_Python/fer2013.csv')\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(f'number of photos in the dataset is: {len(df)}')\n",
    "      \n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for row in df.index:\n",
    "    X.append(list(map(int, df.iloc[row].pixels.split(' '))))\n",
    "    Y.append(df.iloc[row].emotion)\n",
    "\n",
    "X = np.array(X) / 255 # normalise pixel values to lie between 0 and 1\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(f'check number of features is 48**2: {X.shape[1] == 48**2}')\n",
    "\n",
    "X, Y = shuffle(X, Y, random_state=42)  # numpy's shuffle is not nice because you can't shuffle two arrays simultaneously\n",
    "\n",
    "train_proportion = 0.8\n",
    "train_index = int(train_proportion*len(X))\n",
    "\n",
    "X_train, X_test = X[:train_index], X[train_index:]\n",
    "Y_train, Y_test = Y[:train_index], Y[train_index:]\n",
    "# or just use train_test_split from sklearn.model_selection for the same effect\n",
    "n_classes = len(set(Y_train))\n",
    "\n",
    "print(f'Number of samples in training set: {len(X_train)}')\n",
    "print(f'Number of samples in test set: {len(X_test)}')\n",
    "\n",
    "N, D,  = X_train.shape\n",
    "D1 = int(np.sqrt(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.resize((N, D1, D1))\n",
    "X_test.resize((len(X_test), D1, D1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBatchNormalisedLayer(keras.layers.Layer):\n",
    "    def __init__(self, fan_out, decay, act_fun):\n",
    "        super(DenseBatchNormalisedLayer, self).__init__()\n",
    "        # Initialize model parameters\n",
    "        \n",
    "        self.decay = tf.Variable(decay, trainable=False, type=np.float32)\n",
    "\n",
    "        self.act_fun = act_fun\n",
    "        self.fan_out = fan_out\n",
    "  \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(int(input_shape[-1]), self.fan_out), initializer='glorot_normal', trainable=True, name='weight')\n",
    "        self.running_mean = self.add_weight(shape=(1, self.fan_out), initializer='zeros', trainable=False, name='running_mean')\n",
    "        self.running_var = self.add_weight(shape=(1, self.fan_out), initializer='zeros', trainable=False, name='running_var')\n",
    "        \n",
    "        self.gamma = self.add_weight(shape=(1, self.fan_out), initializer='ones', trainable=True, name='gamma')\n",
    "        self.beta = self.add_weight(shape=(1, self.fan_out), initializer='zeros', trainable=True, name='beta')\n",
    "\n",
    "    def call(self, x, is_training):\n",
    "        # flatten input if necessary\n",
    "        if x.ndim > 2:\n",
    "           x = tf.reshape(x, list(x.shape[:-2]) + [x.shape[-2]*x.shape[-1]])\n",
    "\n",
    "        Z = tf.matmul(x, self.W)\n",
    "    \n",
    "        if is_training:\n",
    "            batch_mean, batch_var = tf.nn.moments(Z, [0])\n",
    "            self.running_mean.assign(self.decay*self.running_mean + (1-self.decay)*batch_mean)\n",
    "            self.running_var.assign(self.decay*self.running_var + (1-self.decay)*batch_var)\n",
    "\n",
    "            res = tf.nn.batch_normalization(x=Z, mean=batch_mean, variance=batch_var, offset=self.beta, scale=self.gamma, variance_epsilon=1e-8)\n",
    "\n",
    "        else:\n",
    "            res = tf.nn.batch_normalization(x=Z, mean=self.running_mean, variance=self.running_var, offset=self.beta, scale=self.gamma, variance_epsilon=1e-8)\n",
    "    \n",
    "      \n",
    "        return self.act_fun(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = DenseBatchNormalisedLayer(fan_out=20, decay=0.9, act_fun=tf.nn.relu)\n",
    "_ = layer_norm(np.zeros_like(X_train).reshape((X_train.shape[0], X_train.shape[-1]**2)), is_training=True) # call the layer to build it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm.variables[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(var) for var in layer_norm.variables];\n",
    "print('---------')\n",
    "[print(var) for var in layer_norm.trainable_variables];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model using the subclassing API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(keras.Model):\n",
    "    def __init__(self, n_hidden_units: int = 10, act_fun=tf.nn.relu, decay=0.9):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.batch_norm_layer = DenseBatchNormalisedLayer(fan_out=n_hidden_units, decay=decay, act_fun=act_fun)\n",
    "        self.batch_norm_layer2 = DenseBatchNormalisedLayer(fan_out=n_hidden_units, decay=decay, act_fun=act_fun)\n",
    "        self.decay = decay\n",
    "        self.act_fun = act_fun\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "    \n",
    "    def call(self, x, is_training):\n",
    "        x = self.flatten(x)\n",
    "        x = self.batch_norm_layer(x, is_training=is_training)\n",
    "        x = self.batch_norm_layer2(x, is_training=is_training)\n",
    "        x = tf.keras.layers.Dense(n_classes)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(20, act_fun=tf.nn.relu, decay=0.9)\n",
    "_ = model(np.zeros_like(X_train), is_training=True)\n",
    "\n",
    "# loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# optimiser = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, T):\n",
    "    return tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=T, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dataset = dataset.shuffle(buffer_size=X_train.shape[0]).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "losses = []\n",
    "\n",
    "# Format training loop\n",
    "for epoch in range(epochs):\n",
    "  for x_batch, y_batch in dataset:\n",
    "    x_batch = tf.cast(x_batch, np.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "      batch_loss = loss_function(model(x_batch, is_training=True), y_batch)\n",
    "    # Update parameters with respect to the gradient calculations\n",
    "    grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "    for g,v in zip(grads, model.trainable_variables):\n",
    "        v.assign_sub(learning_rate*g)\n",
    "  # Keep track of model loss per epoch\n",
    "  loss = loss_function(model(tf.cast(X_train, np.float32), is_training=False), Y_train)\n",
    "  losses.append(loss)\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'Cross-entropy loss for step {epoch}: {loss.numpy():0.3f}')\n",
    "\n",
    "# Plot model results\n",
    "print(\"\\n\")\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.title('MSE loss vs training iterations');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
