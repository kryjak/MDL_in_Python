{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sp\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of GPUs detected: {len(tf.config.list_logical_devices('GPU'))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../DL_and_NN_in_Python/fer2013.csv')\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(f'number of photos in the dataset is: {len(df)}')\n",
    "      \n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for row in df.index:\n",
    "    X.append(list(map(int, df.iloc[row].pixels.split(' '))))\n",
    "    Y.append(df.iloc[row].emotion)\n",
    "\n",
    "X = np.array(X) / 255 # normalise pixel values to lie between 0 and 1\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(f'check number of features is 48**2: {X.shape[1] == 48**2}')\n",
    "\n",
    "X, Y = shuffle(X, Y, random_state=42)  # numpy's shuffle is not nice because you can't shuffle two arrays simultaneously\n",
    "\n",
    "train_proportion = 0.8\n",
    "train_index = int(train_proportion*len(X))\n",
    "\n",
    "X_train, X_test = X[:train_index], X[train_index:]\n",
    "Y_train, Y_test = Y[:train_index], Y[train_index:]\n",
    "# or just use train_test_split from sklearn.model_selection for the same effect\n",
    "n_classes = len(set(Y_train))\n",
    "\n",
    "print(f'Number of samples in training set: {len(X_train)}')\n",
    "print(f'Number of samples in test set: {len(X_test)}')\n",
    "\n",
    "N, D,  = X_train.shape\n",
    "D1 = int(np.sqrt(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.resize((N, D1, D1))\n",
    "X_test.resize((len(X_test), D1, D1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def show_sample_image(emotion: int):\n",
    "    emotion_no = emotions.index(emotion)\n",
    "    df_emotion = df[df.emotion == emotion_no]\n",
    "\n",
    "    random_no = np.random.random_integers(0, len(df_emotion))\n",
    "    print(random_no)\n",
    "    print(df_emotion.iloc[random_no])\n",
    "\n",
    "    img = np.array(list(map(int, df_emotion.iloc[random_no].pixels.split(' '))), dtype=np.uint8).reshape((48,48))\n",
    "    img = PIL.Image.fromarray(img).resize((1000, 1000))\n",
    "    img.show()\n",
    "\n",
    "show_sample_image('Surprise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_keras_seq_dense(hidden_layer_sizes: list[int], activation_function: str = 'relu', reg: float = 1e-4):\n",
    "    # instantiate the sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # flatten the image inputs onto a 1D vector\n",
    "    model.add(tf.keras.Input(shape = (D1,D1))) # useful if we want to print the summary of the graph in advance\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for ii in range(len(hidden_layer_sizes)):\n",
    "        # we want the st. dev. at each layer to be 1\n",
    "        # because we assume all vars are IID and normalised such that Var(x_i) = 1\n",
    "        # then to achieve this goal, we need to initialise all weights with st. dev. 1/sqrt(dim)\n",
    "        # where dim = number of multiplications taking place at a given layer to produce a single matrix entry\n",
    "        # for the first layer, this is equal to D - the dimensionality of the inputs\n",
    "        # for subsequent layers, it is equal to the number of hidden units\n",
    "        hidden_units = hidden_layer_sizes[ii]\n",
    "        if ii == 0: \n",
    "            initialiser = tf.keras.initializers.RandomNormal(0, 1/np.sqrt(D))\n",
    "        else:\n",
    "            initialiser = tf.keras.initializers.RandomNormal(0, 1/np.sqrt(hidden_layer_sizes[ii-1]))\n",
    "        regulariser = tf.keras.regularizers.L2(l2=reg)\n",
    "\n",
    "        model.add(Dense(\n",
    "            hidden_units,\n",
    "            activation = activation_function,\n",
    "            use_bias = True,\n",
    "            kernel_initializer = initialiser,\n",
    "            bias_initializer = 'zeros',\n",
    "            kernel_regularizer = regulariser,\n",
    "            bias_regularizer = regulariser\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # add the final layer to project onto n_classes\n",
    "    model.add(Dense(\n",
    "        n_classes,\n",
    "        activation = activation_function,\n",
    "        use_bias = True,\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(0, 1/np.sqrt(hidden_layer_sizes[-1])),\n",
    "        bias_initializer = 'zeros',\n",
    "        kernel_regularizer = regulariser,\n",
    "        bias_regularizer = regulariser\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print('Instantiiated the following model:')\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# this loss function assumes that y_true is NOT one-hot encoded (use CategoricalCrossentropy in this case)\n",
    "# by default, this loss expects the inpu y_pred to be a probabiltiy distribution (i.e. after softmax)\n",
    "# however, we usually don't add the softmax to the end of the graph because it is not stable with all loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras_seq_dense = construct_keras_seq_dense([100, 200, 100], 'relu')\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model_keras_seq_dense.compile(optimizer=optimiser,\n",
    "                              loss=loss_function,\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "model_keras_seq_dense.fit(X_train, Y_train, epochs=500, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_keras_seq_dense.evaluate(X_test, Y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make predictions and evaluate the model manually, add a softmax layer:\n",
    "probability_model = tf.keras.models.Sequential([model_keras_seq_dense, tf.keras.layers.Softmax()])\n",
    "\n",
    "pY = probability_model.predict(X_test)\n",
    "Y_pred = tf.argmax(pY, axis=1)\n",
    "test_acc_manual = np.mean(Y_pred == Y_test)\n",
    "\n",
    "print(test_acc_manual)\n",
    "np.isclose(test_acc, test_acc_manual, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_keras_seq_dense.summary()\n",
    "model_keras_seq_dense.save('model_keras_seq_dense.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.load_model('model_keras_seq_dense.keras')\n",
    "model_loaded.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods of defining a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(10, activation='relu')\n",
    "    self.d2 = Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_subclassing = MyModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model_subclassing.compile(optimizer=optimiser,\n",
    "                              loss=loss_function,\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "model_subclassing.fit(X_train, Y_train, epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_subclassing.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(D1, D1))\n",
    "x = Flatten()(inputs)\n",
    "x = Dense(10, activation='relu', use_bias = True)(x)\n",
    "outputs = Dense(10, activation=None, use_bias = True)(x)\n",
    "\n",
    "model_functional = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model_functional.compile(optimizer=optimiser,\n",
    "                              loss=loss_function,\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "model_functional.fit(X_train, Y_train, epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use model subclassing vs the functional API? Good comparison:\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/functional_api#when_to_use_the_functional_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level implementation without Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a simple dense NN with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLowLevel(tf.Module): # note inheritance from tf.Module, not tf.keras.Model\n",
    "  def __init__(self, M):\n",
    "    # Initialize model parameters\n",
    "    self.W1 = tf.Variable(tf.random.normal((D, M), 0, 1/np.sqrt(D)), type=np.float32)\n",
    "    self.b1 = tf.Variable(tf.zeros(M), type=np.float32)\n",
    "    self.W2 = tf.Variable(tf.random.normal((M, n_classes), 0, 1/np.sqrt(M)), type=np.float32)\n",
    "    self.b2 = tf.Variable(tf.zeros(n_classes), type=np.float32)\n",
    "  \n",
    "  @tf.function\n",
    "  def __call__(self, x): # note __call__, not call\n",
    "    # flatten input if necessary\n",
    "    if x.ndim > 2:\n",
    "      x = tf.reshape(x, list(x.shape[:-2]) + [x.shape[-2]*x.shape[-1]])\n",
    "    Z  = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)\n",
    "    return tf.matmul(Z, self.W2) + self.b2 # return logits - without the softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lowlevel = ModelLowLevel(M=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, T):\n",
    "    return tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=T, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dataset = dataset.shuffle(buffer_size=X_train.shape[0]).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "losses = []\n",
    "\n",
    "# Format training loop\n",
    "for epoch in range(epochs):\n",
    "  for x_batch, y_batch in dataset:\n",
    "    x_batch = tf.cast(x_batch, np.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "      batch_loss = loss_function(model_lowlevel(x_batch), y_batch)\n",
    "    # Update parameters with respect to the gradient calculations\n",
    "    grads = tape.gradient(batch_loss, model_lowlevel.variables)\n",
    "    for g,v in zip(grads, model_lowlevel.variables):\n",
    "        v.assign_sub(learning_rate*g)\n",
    "  # Keep track of model loss per epoch\n",
    "  loss = loss_function(model_lowlevel(tf.cast(X_train, np.float32)), Y_train)\n",
    "  losses.append(loss)\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'Cross-entropy loss for step {epoch}: {loss.numpy():0.3f}')\n",
    "\n",
    "# Plot model results\n",
    "print(\"\\n\")\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.title('MSE loss vs training iterations');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = tf.argmax(tf.nn.softmax(model_lowlevel(tf.cast(X_test, np.float32)), axis=1), axis=1)\n",
    "print(f'error rate: {np.mean(test_preds != Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding sparse_softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]])\n",
    "labels = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
    "print(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)))\n",
    "\n",
    "np.sum(-np.log(tf.nn.softmax(logits, axis=1))*labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]])\n",
    "labels = np.array([0, 1])\n",
    "print(tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)))\n",
    "\n",
    "-np.log(sp.softmax(logits, axis=1))[[0, 1], [0, 1]].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
