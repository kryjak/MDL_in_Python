{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sp\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('mnist_train.csv')\n",
    "# df_test = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "print(f'there are {len(df_train)} samples in the train set')\n",
    "print(f'there are {len(df_test)} samples in the test set')\n",
    "\n",
    "df_train.columns\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='label').to_numpy() / 255\n",
    "X_test = df_test.drop(columns='label').to_numpy() / 255\n",
    "\n",
    "Y_train = df_train['label'].to_numpy()\n",
    "Y_test = df_test['label'].to_numpy()\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_test, Y_test = shuffle(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassUtils(object):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def encode_targets(self, Y):\n",
    "        n_classes = len(set(Y))\n",
    "        T = np.zeros((len(Y), n_classes), dtype=np.int8)\n",
    "        for ii in range(len(Y)):\n",
    "            T[ii, Y[ii]] = 1\n",
    "\n",
    "        return T\n",
    "    \n",
    "    def predict(self, pY):\n",
    "        if pY.ndim > 1:\n",
    "            return np.argmax(pY, axis=1)\n",
    "        else:\n",
    "            return np.argmax(pY)\n",
    "            \n",
    "    def cross_entropy_loss(self, pY, T):\n",
    "        return -np.sum(T * np.log(pY))\n",
    "    \n",
    "    @staticmethod\n",
    "    def classification_rate(preds, Y):\n",
    "        # preds = predict(pY)\n",
    "        return np.mean(preds == Y)\n",
    "    \n",
    "    def forward_pass(self, X, W, b):\n",
    "        pY = sp.softmax(X.dot(W) + b, axis=1)\n",
    "        return pY\n",
    "    \n",
    "    def derivative_W(self, X, pY, T):\n",
    "        return X.T.dot(pY - T)\n",
    "    \n",
    "    def derivative_b(self, pY, T):\n",
    "        return np.sum(pY - T, axis=0)\n",
    "\n",
    "    def get_batches(self, X, Y, batch_size, return_targets: bool = False):\n",
    "        N = len(X)\n",
    "        n_batches = int(np.ceil(N / batch_size))\n",
    "        if return_targets:\n",
    "            T = self.encode_targets(Y)\n",
    "            batches = [[X[i*batch_size: (i+1)*batch_size], Y[i*batch_size: (i+1)*batch_size], T[i*batch_size: (i+1)*batch_size]] for i in range(n_batches)] \n",
    "        else:\n",
    "            batches = [[X[i*batch_size: (i+1)*batch_size], Y[i*batch_size: (i+1)*batch_size]] for i in range(n_batches)] \n",
    "        return batches\n",
    "\n",
    "    def classify(self, X):\n",
    "        return self.predict(self.forward_pass(X, self.W, self.b))\n",
    "        \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        pY = self.forward_pass(X_test, self.W, self.b)\n",
    "        T = self.encode(self.encode_targets(Y_test))\n",
    "        error_rate = 1 - self.classification_rate(self.predict(pY), Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression(MultiClassUtils):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        T = self.encode_targets(Y)\n",
    "        for ii in range(epochs):\n",
    "            pY = self.forward_pass(X, self.W, self.b)\n",
    "\n",
    "            grad_W = self.derivative_W(X, pY, T) + reg*self.W\n",
    "            grad_b = self.derivative_b(pY, T) + reg*self.b\n",
    "            self.W -= learning_rate * grad_W\n",
    "            self.b -= learning_rate * grad_b\n",
    "\n",
    "            if ii % 20 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T))\n",
    "                error_rate = 1 - self.classification_rate(self.predict(pY), Y)\n",
    "\n",
    "                print(f'Cost at epoch {ii}: {cost[-1]}, training error rate: {error_rate}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()\n",
    "    \n",
    "        print(f'Error rate: {error_rate}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassLogisticRegression()\n",
    "model.fit(X_train[:100], Y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionStochastic(MultiClassUtils):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, gradient_method: str = 'SGD', batch_size: int = 8):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        if gradient_method == 'SGD':\n",
    "            for ii in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                T = self.encode_targets(Y)\n",
    "                \n",
    "                for jj in range(N):\n",
    "                    X_sample, T_sample = X[jj].reshape((1, D)), T[jj]\n",
    "                    pY = self.forward_pass(X_sample, self.W, self.b)\n",
    "\n",
    "                    self.W -= learning_rate*(self.derivative_W(X_sample, pY, T_sample) + reg*self.W)\n",
    "                    self.b -= learning_rate*(self.derivative_b(pY, T_sample) + reg*self.b)\n",
    "\n",
    "                if ii % 20 == 0:\n",
    "                    cost.append(self.cross_entropy_loss(pY, T_sample))\n",
    "                    # error_rate = 1 - self.classification_rate(self.predict(pY), Y_sample)\n",
    "                    # pointless to calculate error on a sample size of one\n",
    "\n",
    "                    print(f'SGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        elif gradient_method == 'MBGD':\n",
    "            for ii in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "                for jj in range(len(batches)):\n",
    "                    X_batch, _, T_batch = batches[jj]\n",
    "                    pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                    self.W -= learning_rate * self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                    self.b -= learning_rate * self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                if ii % 50 == 0:\n",
    "                    cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                    # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                    # slightly more meaningful than for SGD, but still pretty useless\n",
    "\n",
    "                    print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stochastic = MultiClassLogisticRegressionStochastic()\n",
    "model_stochastic.fit(X_train[:100], Y_train[:100], gradient_method='MBGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionMomentum(MultiClassUtils):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, momentum_type: str = 'normal', batch_size: int = 8, mu: float = 0.0):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        velocity_W = 0 # set initial velocities to 0\n",
    "        velocity_b = 0\n",
    "\n",
    "        for ii in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "            for jj in range(len(batches)):\n",
    "                X_batch, _, T_batch = batches[jj]\n",
    "                pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                grad_W = self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                grad_b = self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                if momentum_type == 'normal':\n",
    "                    # normal momentum\n",
    "                    velocity_W = mu*velocity_W - learning_rate * grad_W\n",
    "                    velocity_b = mu*velocity_b - learning_rate * grad_b\n",
    "\n",
    "                    self.W += velocity_W\n",
    "                    self.b += velocity_b\n",
    "\n",
    "                elif momentum_type == 'Nestorov':\n",
    "                    # Nestorov momentum\n",
    "                    velocity_W = mu*velocity_W - learning_rate*grad_W\n",
    "                    velocity_b = mu*velocity_b - learning_rate*grad_b\n",
    "\n",
    "                    self.W += mu*velocity_W - learning_rate*grad_W\n",
    "                    self.b += mu*velocity_b - learning_rate*grad_b\n",
    "\n",
    "            if ii % 50 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                # slightly more meaningful than for SGD, but still pretty useless\n",
    "\n",
    "                print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_momentum = MultiClassLogisticRegressionMomentum()\n",
    "model_momentum.fit(X_train[:100], Y_train[:100], momentum_type='Nestorov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionAdaptiveRate(MultiClassUtils):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, batch_size: int = 8, adaptive_learning_rate: str = None):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        T = self.encode_targets(Y)\n",
    "        cache_W = 1\n",
    "        cache_b = 1\n",
    "        eps = 1e-8\n",
    "        decay = 0.99 \n",
    "\n",
    "        for ii in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "            for jj in range(len(batches)):\n",
    "                X_batch, _, T_batch = batches[jj]\n",
    "                pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                grad_W = self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                grad_b = self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                # adaptive learning rates\n",
    "                if adaptive_learning_rate == 'AdaGrad':\n",
    "                    cache_W += grad_W**2\n",
    "                    cache_b += grad_b**2\n",
    "\n",
    "                    self.W -= learning_rate / np.sqrt(cache_W + eps) * grad_W\n",
    "                    self.b -= learning_rate / np.sqrt(cache_b + eps) * grad_b\n",
    "            \n",
    "                elif adaptive_learning_rate == 'RMSProp':\n",
    "                    cache_W = decay*cache_W + (1-decay)*grad_W**2\n",
    "                    cache_b = decay*cache_b + (1-decay)*grad_b**2\n",
    "\n",
    "                    self.W -= learning_rate / np.sqrt(cache_W + eps) * grad_W\n",
    "                    self.b -= learning_rate / np.sqrt(cache_b + eps) * grad_b\n",
    "            \n",
    "                else: # without adaptive learning rate\n",
    "                    self.W -= learning_rate * grad_W\n",
    "                    self.b -= learning_rate * grad_b\n",
    "\n",
    "            if ii % 50 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                # slightly more meaningful than for SGD, but still pretty useless\n",
    "                print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaptive = MultiClassLogisticRegressionAdaptiveRate()\n",
    "model_adaptive.fit(X_train[:100], Y_train[:100], adaptive_learning_rate='RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression_ADAM(MultiClassLogisticRegression):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ADAM = MultiClassLogisticRegression_ADAM()\n",
    "model_ADAM.fit(X_train[:100], Y_train[:100], epochs=1000, gradient_method='FGD', mu=0.9, adaptive_learning_rate='AdaGrad')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
