{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains demonstrations of how to do:\n",
    "1. Stochastic gradient descent\n",
    "2. Momentum (normal and Nestorov)\n",
    "3. Adaptive learning rate (AdaGrad and RMSprop)\n",
    "4. Adam\n",
    "\n",
    "For simplicity, we use multi-class logistic regression (no hidden layers). All these procedures can be easily extended to proper NNs with hidden layers - usually it's just a case of adding more variables for momentum, cache, etc. (one for each weight and bias).\n",
    "\n",
    "Data from: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as sp\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_classes import LogisticRegressionUtils, MultiClassUtils, BatchUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('mnist_train.csv')\n",
    "# df_test = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "print(f'there are {len(df_train)} samples in the train set')\n",
    "print(f'there are {len(df_test)} samples in the test set')\n",
    "\n",
    "df_train.columns\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='label').to_numpy() / 255\n",
    "X_test = df_test.drop(columns='label').to_numpy() / 255\n",
    "\n",
    "Y_train = df_train['label'].to_numpy()\n",
    "Y_test = df_test['label'].to_numpy()\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_test, Y_test = shuffle(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression(LogisticRegressionUtils, MultiClassUtils, BatchUtils):\n",
    "    def __init__(self) -> None:\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        T = self.encode_targets(Y)\n",
    "        for ii in range(epochs):\n",
    "            pY = self.forward_pass(X, self.W, self.b)\n",
    "\n",
    "            grad_W = self.derivative_W(X, pY, T) + reg*self.W\n",
    "            grad_b = self.derivative_b(pY, T) + reg*self.b\n",
    "            self.W -= learning_rate * grad_W\n",
    "            self.b -= learning_rate * grad_b\n",
    "\n",
    "            if ii % 20 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T))\n",
    "                error_rate = 1 - self.classification_rate(self.predict(pY), Y)\n",
    "\n",
    "                print(f'Cost at epoch {ii}: {cost[-1]}, training error rate: {error_rate}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()\n",
    "    \n",
    "        print(f'Error rate: {error_rate}')\n",
    "        \n",
    "    def classify(self, X):\n",
    "        return self.predict(self.forward_pass(X, self.W, self.b))\n",
    "        \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        preds = self.classify(X_test, self.W, self.b)\n",
    "        T = self.encode(self.encode_targets(Y_test))\n",
    "        error_rate = 1 - self.classification_rate(preds, T)\n",
    "        return error_rate   \n",
    "    \n",
    "    # def predict(self, pY):\n",
    "        # return super().predict(pY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassLogisticRegression()\n",
    "model.fit(X_train[:100], Y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionStochastic(MultiClassLogisticRegression):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, gradient_method: str = 'SGD', batch_size: int = 8):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        if gradient_method == 'SGD':\n",
    "            for ii in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                T = self.encode_targets(Y)\n",
    "                \n",
    "                for jj in range(N):\n",
    "                    X_sample, T_sample = X[jj].reshape((1, D)), T[jj]\n",
    "                    pY = self.forward_pass(X_sample, self.W, self.b)\n",
    "\n",
    "                    self.W -= learning_rate*(self.derivative_W(X_sample, pY, T_sample) + reg*self.W)\n",
    "                    self.b -= learning_rate*(self.derivative_b(pY, T_sample) + reg*self.b)\n",
    "\n",
    "                if ii % 20 == 0:\n",
    "                    cost.append(self.cross_entropy_loss(pY, T_sample))\n",
    "                    # error_rate = 1 - self.classification_rate(self.predict(pY), Y_sample)\n",
    "                    # pointless to calculate error on a sample size of one\n",
    "\n",
    "                    print(f'SGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        elif gradient_method == 'MBGD':\n",
    "            for ii in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "                for jj in range(len(batches)):\n",
    "                    X_batch, _, T_batch = batches[jj]\n",
    "                    pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                    self.W -= learning_rate * self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                    self.b -= learning_rate * self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                if ii % 50 == 0:\n",
    "                    cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                    # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                    # slightly more meaningful than for SGD, but still pretty useless\n",
    "\n",
    "                    print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stochastic = MultiClassLogisticRegressionStochastic()\n",
    "model_stochastic.fit(X_train[:100], Y_train[:100], gradient_method='MBGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionMomentum(MultiClassLogisticRegression):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, momentum_type: str = 'normal', batch_size: int = 8, mu: float = 0.0):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        velocity_W = 0 # set initial velocities to 0\n",
    "        velocity_b = 0\n",
    "\n",
    "        for ii in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "            for jj in range(len(batches)):\n",
    "                X_batch, _, T_batch = batches[jj]\n",
    "                pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                grad_W = self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                grad_b = self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                if momentum_type == 'normal':\n",
    "                    # normal momentum\n",
    "                    velocity_W = mu*velocity_W - learning_rate * grad_W\n",
    "                    velocity_b = mu*velocity_b - learning_rate * grad_b\n",
    "\n",
    "                    self.W += velocity_W\n",
    "                    self.b += velocity_b\n",
    "\n",
    "                elif momentum_type == 'Nestorov':\n",
    "                    # Nestorov momentum\n",
    "                    velocity_W = mu*velocity_W - learning_rate*grad_W\n",
    "                    velocity_b = mu*velocity_b - learning_rate*grad_b\n",
    "\n",
    "                    self.W += mu*velocity_W - learning_rate*grad_W\n",
    "                    self.b += mu*velocity_b - learning_rate*grad_b\n",
    "\n",
    "            if ii % 50 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                # slightly more meaningful than for SGD, but still pretty useless\n",
    "\n",
    "                print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_momentum = MultiClassLogisticRegressionMomentum()\n",
    "model_momentum.fit(X_train[:100], Y_train[:100], momentum_type='Nestorov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionAdaptiveRate(MultiClassLogisticRegression):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, batch_size: int = 8, adaptive_learning_rate: str = None):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        T = self.encode_targets(Y)\n",
    "        cache_W = 1\n",
    "        cache_b = 1\n",
    "        eps = 1e-8\n",
    "        decay = 0.99 \n",
    "\n",
    "        for ii in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "            for jj in range(len(batches)):\n",
    "                X_batch, _, T_batch = batches[jj]\n",
    "                pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                grad_W = self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                grad_b = self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                # adaptive learning rates\n",
    "                if adaptive_learning_rate == 'AdaGrad':\n",
    "                    cache_W += grad_W**2\n",
    "                    cache_b += grad_b**2\n",
    "\n",
    "                    self.W -= learning_rate / np.sqrt(cache_W + eps) * grad_W\n",
    "                    self.b -= learning_rate / np.sqrt(cache_b + eps) * grad_b\n",
    "            \n",
    "                elif adaptive_learning_rate == 'RMSProp':\n",
    "                    cache_W = decay*cache_W + (1-decay)*grad_W**2\n",
    "                    cache_b = decay*cache_b + (1-decay)*grad_b**2\n",
    "\n",
    "                    self.W -= learning_rate / np.sqrt(cache_W + eps) * grad_W\n",
    "                    self.b -= learning_rate / np.sqrt(cache_b + eps) * grad_b\n",
    "            \n",
    "                else: # without adaptive learning rate\n",
    "                    self.W -= learning_rate * grad_W\n",
    "                    self.b -= learning_rate * grad_b\n",
    "\n",
    "            if ii % 50 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                # slightly more meaningful than for SGD, but still pretty useless\n",
    "                print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adaptive = MultiClassLogisticRegressionAdaptiveRate()\n",
    "model_adaptive.fit(X_train[:100], Y_train[:100], adaptive_learning_rate='RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionADAM(MultiClassLogisticRegression):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, X, Y, epochs=10**3, learning_rate=1e-5, reg=0, batch_size: int = 8, beta1 = 0, beta2 = 0):\n",
    "        N, D = X.shape\n",
    "        # initialise weights and biases:\n",
    "        n_classes = len(set(Y))\n",
    "    \n",
    "        self.W = rng.normal(size=(D, n_classes)) / np.sqrt(D)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "    \n",
    "        cost = []\n",
    "        error = []\n",
    "    \n",
    "        mW_t = 0 # initialise momentum\n",
    "        mb_t = 0\n",
    "        vW_t = 0 # initialise cache\n",
    "        vb_t = 0\n",
    "\n",
    "        t = 1 # initialise timestep\n",
    "        eps = 1e-8\n",
    "\n",
    "        T = self.encode_targets(Y)\n",
    "        \n",
    "        for ii in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            batches = self.get_batches(X, Y, batch_size, return_targets=True)\n",
    "\n",
    "            for jj in range(len(batches)):\n",
    "                X_batch, _, T_batch = batches[jj]\n",
    "                pY = self.forward_pass(X_batch, self.W, self.b)\n",
    "\n",
    "                grad_W = self.derivative_W(X_batch, pY, T_batch) + reg*self.W\n",
    "                grad_b = self.derivative_b(pY, T_batch) + reg*self.b\n",
    "\n",
    "                mW_t = beta1*mW_t + (1-beta1)*grad_W # update momenta\n",
    "                mb_t = beta1*mb_t + (1-beta1)*grad_b\n",
    "                vW_t = beta2*vW_t + (1-beta2)*grad_W**2 # update cache\n",
    "                vb_t = beta2*vb_t + (1-beta2)*grad_b**2\n",
    "\n",
    "                mWhat_t = mW_t / (1-beta1**t) # define bias-corrected first moment estimate of the gradient\n",
    "                mbhat_t = mb_t / (1-beta1**t)\n",
    "                vWhat_t = vW_t / (1-beta2**t) # define bias-corrected second moment estimate of the gradient\n",
    "                vbhat_t = vb_t / (1-beta2**t)\n",
    "\n",
    "                self.W -= learning_rate*mWhat_t / (np.sqrt(vWhat_t) + eps)\n",
    "                self.b -= learning_rate*mbhat_t / (np.sqrt(vbhat_t) + eps)\n",
    "            \n",
    "                t +=1 # update timestep\n",
    "\n",
    "            if ii % 50 == 0:\n",
    "                cost.append(self.cross_entropy_loss(pY, T_batch))\n",
    "                # error_rate = 1 - self.classification_rate(self.predict(pY), Y_batch)\n",
    "                # slightly more meaningful than for SGD, but still pretty useless\n",
    "                print(f'MBGD. Cost at epoch {ii}: {cost[-1]}')\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ADAM = MultiClassLogisticRegressionADAM()\n",
    "model_ADAM.fit(X_train[:100], Y_train[:100], epochs=1000, learning_rate=0.001, beta1=0.9, beta2=0.999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
